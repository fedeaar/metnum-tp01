% === demo 1 === %
\subsection{A: $A = pWD + ez^t$}\label{A.1}
\begin{proof}[demostración] 
    
Recordemos que:

\begin{align*}
    e_i     &=  1
    \\
    \\
    z_{j}   &=  \left\{ 
                    \begin{array}{lcc}
                    (1 - p) / n     &  \ \text{si}    &  c_j \neq 0 \\
                    1 / n           &  \ \text{si no} &
                    \end{array}
                \right.\
    \\
    \\
    w_{ij}  &=  \left\{ 
                    \begin{array}{lcc}
                    1               &  \qquad \qquad \text{si}    & i \neq j\  \wedge\ j \stackrel{l}{\longrightarrow} i \\
                    0               &  \qquad \qquad \text{si no} &
                    \end{array}
                \right.\
    \\
    \\
    d_{ij}  &=  \left\{ 
                    \begin{array}{lcc}
                    1 / c_j         &  \qquad \: \: \text{si}    & i = j\  \wedge\ c_j \neq 0 \\
                    0               &  \qquad \ \  \text{si no} &
                    \end{array}
                \right.\
\end{align*}
\vspace{1em}

\noindent A partir de estas definiciones, vemos que, como $\textbf{D}$ es diagonal, el producto a derecha $\textbf{W}\textbf{D}$ escala cada columna $w_j$ por el factor $d_{jj}$, tal que:
\vspace{1em}

\begin {equation*}
    (\textbf{W}\textbf{D})_{ij}  =  \left\{ 
                    \begin{array}{lcc}
                    w_{ij} / c_j    & \ \ \ \ \ \text{si}    & c_j \neq 0 \\
                    0               & \ \ \ \ \ \text{si no} &
                    \end{array}
                \right.\
\end {equation*}
\vspace{1em}

\noindent Como $p$ es un escalar, sigue entonces que:
\vspace{1em}

\begin {equation*}
    (p\textbf{W}\textbf{D})_{ij}  =   \left\{ 
                        \begin{array}{lcc}
                        p \cdot w_{ij} / c_j    &  \text{si}    & c_j \neq 0 \\
                        0                       &  \text{si no} &
                        \end{array}
                    \right.\
\end {equation*}
\vspace{1em}

\noindent Además, $\ e \in \mathbb{R}^{n \times 1}\ \wedge\ z^t \in \mathbb{R}^{1 \times n} \implies ez^t \in \mathbb{R}^{n \times n}\ $, y:
\vspace{1em}

\begin {equation*}
    (ez^t)_{ij} := \sum_{k=1}^{1} e_{ik} \cdot z^t_{kj} = e_i \cdot z^t_j = 1 \cdot z^t_j = z_j 
\end {equation*}
\vspace{1em}

\noindent Por lo que:
\begin {align*}
    (p\textbf{W}\textbf{D} + ez^t)_{ij}   &=   \left\{ 
                                \begin{array}{lcc}
                                p \cdot w_{ij} / c_j + z_j   &  \ \ \ \ \qquad \text{si}    & c_j \neq 0 \\
                                z_j                          &  \ \ \ \ \qquad \text{si no} &
                                \end{array}
                            \right.\ \\
                            \\
                        &=  \left\{ 
                                \begin{array}{lcc}
                                (1 - p) \cdot \frac{1}{n} + p \cdot \frac{w_{ij}}{c_j}   &  \ \ \   \text{si}    & c_j \neq 0 \\
                                \frac {1}{n}                                             &  \ \ \   \text{si no} &
                                \end{array}
                            \right.\
\end {align*}

\noindent pero: 
\vspace{1em}

\begin{equation*}
    a_{ij} := Pr(j \longrightarrow i) = \left\{ 
                                            \begin{array}{lcc}
                                            (1 - p)\cdot \frac{1}{n} + p \cdot \frac{I_{ij}}{c_j}      &  \text{si}    & c_j \neq 0\\
                                            \frac{1}{n}                                                &  \text{si no}  &
                                            \end{array}
                                        \right.
\end{equation*}
\vspace{1em}

Como $\ I_{ij} = 1\ $ si y sólo si existe un hipervínculo de $j$ a $i$, con $j \neq i$ ---y nulo en caso contrario---, entonces $\ I_{ij} = w_{ij}\ $ y concluímos que $\ a_{ij} = (p\textbf{W}\textbf{D} + ez^t)_{ij}$ $\ \forall i, j:\ 1\ ...\ n\ $,  lo que implica que:
\vspace{1em}

\begin{equation*}
    \textbf{A} = p\textbf{W}\textbf{D} + ez^t
\end{equation*}
\vspace{1em}

\end{proof}



% === demo 2 === %
\newpage
\subsection{B: $I - pWD$ permite la eliminación gaussiana}\label{A.2}

\begin{proof}[Demostración] Demos primero una definición formal del enunciado:  
    
\vspace{2em}
\noindent \textsc{Eliminación Gaussiana}. $\forall \mathbf{A} \in \mathbb{R}^{n \times n},\ \exists!\ m \in \mathbb{N}:\ 1 \leq m \leq n$ tal que: 

\vspace{1em}
\noindent $\forall k \in \mathbb{N}:\ 1 \leq k < m$

\vspace{0.5em}
\begin{align*}
    eg_{1}(\mathbf{A})_{ij}  &= a_{ij}
    \\
    eg_{k+1}(\mathbf{A})_{ij} &=  
        \left\{ 
            \begin{array}{lcc}
                eg_{k}(\mathbf{A})_{ij}  &   \text{si}    & i < k+1 \ \vee \  j < k+1 \\
                eg_{k}(\mathbf{A})_{ij} - \frac {eg_{k}(\mathbf{A})_{ik}}{eg_{k}(\mathbf{A})_{kk}}\cdot \ eg_{k}(\mathbf{A})_{kj} & \text{si no} \\
            \end{array}
        \right.\ 
\end{align*}  

\vspace{1em}    
\noindent donde $m < n$ es el mínimo valor que satisface que $eg_{m}{(\mathbf{A})}_{mm} = 0$. Si la matríz $\mathbf{A}$ es tal que $m = n$, entonces $\mathbf{A}$ permite la eliminación gaussiana.

\vspace{3em}
\noindent Probaremos que para la matríz $\ \mathbf{B} \in \mathbb{R}^{n \times n} = \mathbf{I} - p\mathbf{W}\mathbf{D}\ $, $\ \forall k \in \mathbb{N}: 1 \leq k \leq n\ $: 

\begin{equation*}
eg_{k}{(\mathbf{B})}_{kk} \neq 0
\end{equation*}

\vspace{0.5em}
\noindent y en consecuencia $m = n$.


\vspace{2em}
\noindent Para ello demostraremos el siguiente enunciado:

\begin{equation*}
    |b^{k}_{jj}| \ > \sum_{i = k,\ i \neq j}^{n} |b^{k}_{ij}| \qquad \forall k,\ j \in \mathbb{N}: 1 \leq k \leq j \leq n
\end{equation*}

\vspace{0.5em}
\noindent donde $b^k_{ij} = eg_k(\mathbf{B})_{ij}$.

\vspace{1em}
Es decir, para todo paso $k$ de la eliminación gaussiana se satisface que el valor absoluto de los elementos de la diagonal mayores o iguales a $k$ son mayor estricto al valor absoluto de la suma de los elementos de su columna desde la posición $k$, descontando la diagonal.

\vspace{1em}
\noindent Lo demostraremos por inducción:

\vspace{2em}
\noindent $\forall k,\ j \in \mathbb{N}: 1 \leq k \leq j \leq n$ 

\begin{equation*}
    |b^{k}_{jj}| \ > \sum_{i = k,\ i \neq j}^{n} |b^{k}_{ij}| \implies |b^{k+1}_{jj}| \ > \sum_{i = k+1,\ i \neq j}^{n} |b^{k+1}_{ij}|
\end{equation*}

\newpage
\noindent \textsc{Caso base} ($k = 1$):

\vspace{1em}
\noindent Notemos primero que 

\begin{equation*}
    (\mathbf{B})_{ij} = (\mathbf{I} - p\mathbf{W}\mathbf{D})_{ij} =
    \left\{ 
        \begin{array}{lcc}
            1           & \text{si} & i = j   \\
            -p / c_j    & \text{si} & w_{ij} = 1\ \wedge\ c_j \neq 0 \\
            0           & \text{si no}   
        \end{array}
    \right.\ 
\end{equation*}

\vspace{2em}
Dado que todo elemento de cada columna $j$ de $\mathbf{W}\mathbf{D}$ está normalizado por $1/c_j$, tal que su suma da uno, entonces la suma de la columna $(-p\mathbf{W}\mathbf{D})_{j} = -p$, con $0 < p < 1$.

\vspace{1em}
Observamos también que la diagonal es nula para esta matríz, tal que su suma por $\mathbf{I}$ da $(\mathbf{I} - p\mathbf{W}\mathbf{D})_{jj} = 1$. Este tipo de matríz se conoce como estocástica por columna y su transpuesta es diagonal dominante. 

\vspace{1em}
\noindent En consecuencia, es inmediato que $\ |b^{1}_{jj}| = |b_{jj}| = 1\ >\ \sum_{i = k,\ i \neq j}^{n} |b^{1}_{ij}| = |-p|\ \forall j \in \mathbb{N}: 1 \leq j \leq n$. 

\hfill$\square$


\vspace{2em}
\noindent \textsc{Paso inductivo}:

\vspace{1em}
\noindent $\forall j \in \mathbb{N}: 1 \leq k \leq j \leq n$

\begin{align*}
    |b^{k+1}_{jj}|\ 
        &>\ \sum_{i = k+1,\ i \neq j}^{n} |b^{k+1}_{ij}| \\
    \iff |b^k_{jj} - \frac {b^k_{jk} \ \cdot \ b^k_{kj}}{b^k_{kk}}|\     
        &>\ \sum_{i=k+1,\ i \neq j}^{n} |b^k_{ij} - \frac {b^k_{ik} \ \cdot \ b^k_{kj}}{b^k_{kk}}|
\end{align*}

\vspace{1em}
\noindent dado que $|a - b| \geq |a| - |b|\ \wedge\ |a| + |b| \geq |a + b|$ por desigualdad triangular, entonces podemos demostrar equivalentemente, por teorema del sandwich, que:

%\vspace{1em}
\begin{equation*}
    |b^k_{jj}| - |\frac {b^k_{jk} \ \cdot \ b^k_{kj}}{b^k_{kk}}|\  
        >\ \sum_{i=k+1,\ i \neq j}^{n} |b^k_{ij}|\ \ \  +  \sum_{i=k+1,\ i \neq j}^{n} |\frac{b^k_{ik} \ \cdot \ b^k_{kj}}{b^k_{kk}}| \\
\end{equation*} 

\vspace{1em}
\noindent Como:

%\vspace{1em}
\begin{equation*}
    \sum_{i=k+1,\ i \neq j}^{n} |b^k_{ij}|\ 
        =\ (\sum_{i=k, \ i \neq j}^{n} |b^k_{ij}|) -  |b^k_{kj}|
\end{equation*}

\vspace{1em}
\noindent y por hipótesis inductiva:

%\vspace{1em}
\begin{equation*}
    |b^k_{jj}|\
        >\ \sum_{i=k, \ i \neq j}^{n} |b^k_{ij}|\
    \implies\
    |b^k_{jj}| - |b^k_{kj}|\
        >\ (\sum_{i=k, \ i \neq j}^{n} |b^k_{ij}|) -  |b^k_{kj}|
\end{equation*}

\vspace{1em}
\noindent entonces podemos volver a acotar tal que:

%\vspace{1em}
\begin{equation*}
    |b^k_{jj}| - |b^k_{kj}|\ +  \sum_{i=k+1, \ i \neq j}^{n} |\frac{b^k_{ik} \ \cdot \ b^k_{kj}}{b^k_{kk}}|\
        >_{hi}\ \sum_{i=k+1,\ i \neq j}^{n} |b^k_{ij}|\ +  \sum_{i=k+1,\ i \neq j}^{n} |\frac{b^k_{ik} \ \cdot \ b^k_{kj}}{b^k_{kk}}| \\
\end{equation*}

\vspace{1em}
\noindent y considerar que:

%\vspace{1em}
\begin{align*}
    |b^k_{jj}| - |\frac {b^k_{jk} \ \cdot \ b^k_{kj}}{b^k_{kk}}|\ 
        &\geq\ |b^k_{jj}| - |b^k_{kj}| +  \sum_{i=k+1, \ i \neq j}^{n} |\frac{b^k_{ik} \ \cdot \ b^k_{kj}}{b^k_{kk}}|\ \\  
    \iff |b^k_{kj}| - |\frac {b^k_{jk}\ \cdot \ b^k_{kj}}{b^k_{kk}}|\ 
        &\geq\ \frac{ |b^k_{kj}|}{|b^k_{kk}|} \sum_{i=k+1, \ i \neq j}^{N} |b^k_{ik}|
\end{align*}

\vspace{1em}
\noindent Podemos acotar una última vez, tal que:

%\vspace{2em}
\begin{equation*}
    |b^k_{kj}| - |\frac {b^k_{jk}\ \cdot \ b^k_{kj}}{b^k_{kk}}|\ 
        \geq\ \frac{|b^k_{kj}|}{|b^k_{kk}|} (|b^k_{kk}| - |b^k_{jk}|)\
        >_{hi}\ \frac{ |b^k_{kj}|}{|b^k_{kk}|} \sum_{i=k+1, \ i \neq j}^{N} |b^k_{ik}|
\end{equation*}

\vspace{1em}
\noindent Pero:

%\vspace{1em}
\begin{align*}
    |b^k_{kj}| - |\frac {b^k_{jk}\ \cdot \ b^k_{kj}}{b^k_{kk}}|\ 
        \geq\ \frac{|b^k_{kj}|}{|b^k_{kk}|} (|b^k_{kk}| - |b^k_{jk}|)\
    \iff |b^k_{kj}| - |\frac {b^k_{jk}\ \cdot\ b^k_{kj}}{b^k_{kk}}| 
        &\geq  |b^k_{kj}| - |\frac{b^k_{jk}\ \cdot \ b^k_{kj}}{b^k_{kk}}|\\
\end{align*}

\vspace{1em}
\noindent Lo que es trivialmente cierto. 

\hfill$\square$


\vspace{2em}
\noindent Demostramos ahora por absurdo que $b^{k}_{kk} \neq 0\ \ \forall k \in \mathbb{N}: 1 \leq k \leq n\ $.

\vspace{1em}
\noindent Supongamos que $b^{k}_{kk} = 0$. Entonces:

%\vspace{1em}
\begin{equation*}
    |b^{k}_{kk}| \ =\ 0\ >\ \sum_{i = k,\ i \neq k}^{n} |b^{k}_{ij}| \qquad \forall k,\ j \in \mathbb{N}: 1 \leq k \leq j \leq n
\end{equation*}

\vspace{1em}
\noindent Pero $\sum_{i = k,\ i \neq k}^{n} |b^{k}_{ij}|$ es necesariamente una suma de valores positivos ó $0$. Es decir: 

%\vspace{1em}
\begin{equation*}
    0\ > \sum_{i = k,\ i \neq k}^{n} |b^{k}_{ij}|\ \geq\ 0\ \implies\ 0 > 0
\end{equation*}

\vspace{1em}
\noindent Absurdo! 

\end{proof}
 



\newpage
% === estructuras alternativas === %
\subsection{C: Estructuras alternativas}\label{A.3}